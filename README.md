# Kmeans

1、什么是K-means：所谓k-means就是指将一堆没有标签的数据自动的划分成几类的方法，这个方法要保证同一类的数据是有相似特征的

2、k-means又叫做k均值，是因为这种算法可以发现k个不同的簇，且每个簇的中心采用簇中所含值得均值计算得到

3、聚类与分类得最大得不同在于，分类得目标是事先已经知道的，而聚类则不同，因为其产生的结果与分类相同，而只是类别没有预先定义，聚类有时也被称为无监督学习

4、k-means的具体过程是：

（1）随机生成k个簇类质心

（2）计算每个数据点到k个簇类质心之间的距离

（3）根据每个点到k个簇类质心的距离选择最短距离的那个簇类质心，然后就将该点归到该簇类质心下

（4）归类结束后，根据每个簇的点根据平均值计算中心点，来重新选定簇的质心

（5）重复（2）、（3）、（4），直到簇类中心点不在变动时就表示算法收敛了

5、点与点之间的距离公式：

（1）![image](https://github.com/zjzj1992/Kmeans/blob/master/images/kjuli.gif)：λ的值是可以任意取的

（2）![image](https://github.com/zjzj1992/Kmeans/blob/master/images/k-oushi.gif)：这个是欧式距离

（3）![image](https://github.com/zjzj1992/Kmeans/blob/master/images/k-manha.gif)：这个是曼哈顿距离

6、K-means算法的特点是类别的个数是由人为给定的

7、K-means测量数据之间的相似度可以采用欧氏距离，但是如果不能使用欧氏距离度量的话，就必须要先把数据转化为能用欧氏距离度量的，否则聚类结果就会出错
举一个没有转换数据结果使用了欧氏距离后聚类结果出错的例子：

![image](https://github.com/zjzj1992/Kmeans/blob/master/images/k-liuxing.jpg)

这张图是一个瑞士卷形状的流形，针对这样的数据如果要比较相似性就应该使用测地距离，测地距离就是在沿着曲面由A到B之间的最短距离，但是很明显这个图显示的聚类效果并不是使用测地距离，而是欧氏距离，所以聚类结果出现了问题

8、k-means的优点：容易实现

            缺点：局部最优、容易收到初始质心的影响（初始质心选择不当会导致次优的聚类结果，SSE较大）、k值的选取（应与数据本身结构相吻合，但是很难把握，所以选取的最优k值是很难的）、在大规模数据集上收敛较慢

9、存在一种方式可以大概确定k值是多少，具体说就是，根据代价函数和k的数目之间的关系，选取拐点处的k值，但是在实际情况中这是很少出现的，比较好的方式还是要从实际问题出发，人工的指定一个比较合理的k值，然后通过多次随机初始化质心来选取结果最好的那个

10、伪代码
创建k个点作为起始质心（通常是随机的）
当任意一个点的簇分配结果发生改变时
        对数据集中的每个数据点
                对每个质心
                        计算质心与数据点之间的距离
                将数据点分配到距其最近的簇
        对每一个簇，计算簇中所有点的均值并将均值作为新质心

11、其中的随机质心必须要在整个数据集的边界之内，这个可以通过找到数据集中每一维的最小值和最大值来完成

12、k-均值算法收敛但聚类效果较差的原因是，k-均值算法收敛到了局部最小值，而非全局最小值（局部最小值是指结果还可以但是并非最好的，而全局最小值可能是最好的结果）

13、用于度量聚类效果的指标是：

![image](https://github.com/zjzj1992/Kmeans/blob/master/images/k-sse.jpg)


（1）x是样本，C是质心

（2）SSE越小就表明数据点越接近于质心，聚类效果也就越好。

（3）因为SSE采用了误差平方和，所以这种方式更加重视那些远离中心的点。

（4）有一种肯定可以降低SSE值的方式就是增加簇的个数，但是这违背了聚类的目标

（5）聚类的目标是在保持簇类数目不变的前提下提高簇的质量

14、聚类结束后可以使用后处理来提高聚类性能，方法是：

（1）将具有最大SSE值的簇划分成两个簇。具体实现大概是，将最大簇中包含的数据过滤出来，然后使用这些点运行k-均值算法，其中k=2

（2）但是通过上面的方式后，簇类数目会增加，所以为了保持簇总数不变，还可以将某两个簇进行合并。在数据很简单的情况下，使用可视化可以直接观察到数据的分布情况，然后直接进行合并。但是当数据很复杂的时候，可视化就变得很复杂了，所以可以计算所有质心之间的距离，然后将距离最近的两个质心进行合并。还有一种方式是，对所有的簇进行两两合并，然后分别计算总的SSE值，直到找到最佳的合并方式

15、为了解决k-means局部收敛的问题，可以采用二分k-means算法，其主要思想就是：
首先将所有点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值。然后一直重复直到得到用户指定的簇数目。

16、二分K-均值伪代码：

将所有点看成一个簇

当粗数目小于k时
        
        对于每一个簇
                
                计算总误差
                
                在给定的簇上面进行K-均值聚类（k=2）
                
                计算将该簇一分为二后的总误差
        
        选择使得误差最小的那个簇进行划分
